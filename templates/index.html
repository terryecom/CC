import threading
import time
import socket
import ssl
import os
from urllib.parse import urlparse, urljoin, urldefrag
from datetime import datetime

import requests
import whois
from bs4 import BeautifulSoup
from flask import Flask, render_template, request, jsonify, send_file, session
import dns.resolver

app = Flask(__name__)
app.secret_key = os.environ.get('FLASK_SECRET', 'terry_ecom_linkchecker')

EXCLUDED_DOMAINS = [
    'g.co', 'facebook.com', 'instagram.com', 'x.com', 'twitter.com',
    'pinterest.com', 'shopify.com', 'edpb.europa.eu'
]

crawl_states = {}

def normalize_domain(domain):
    return domain.lower().replace("www.", "")

def get_domain_info(domain):
    # WHOIS info with robust parsing
    try:
        w = whois.whois(domain)
        created = w.creation_date
        if isinstance(created, list): created = created[0]
        updated = w.updated_date
        if isinstance(updated, list): updated = updated[0]
        expires = w.expiration_date
        if isinstance(expires, list): expires = expires[0]
        registrar = w.registrar
        org = w.org if hasattr(w, "org") else None
        nameservers = w.name_servers
        if isinstance(nameservers, str):
            nameservers = [nameservers]
        elif nameservers is None:
            nameservers = []
        nameservers = [str(ns).strip() for ns in nameservers if ns]
        return {
            "domain": domain,
            "created": str(created) if created else "Unknown",
            "updated": str(updated) if updated else "Unknown",
            "expires": str(expires) if expires else "Unknown",
            "registrar": registrar if registrar else "Unknown",
            "org": org if org else "Unknown",
            "nameservers": nameservers if nameservers else ["Unknown"]
        }
    except Exception:
        return {
            "domain": domain,
            "created": "Unknown",
            "updated": "Unknown",
            "expires": "Unknown",
            "registrar": "Unknown",
            "org": "Unknown",
            "nameservers": ["Unknown"]
        }

def get_dns_records(domain):
    # Returns A, AAAA, MX records
    try:
        answers_a = [r.address for r in dns.resolver.resolve(domain, 'A', lifetime=5)]
    except Exception:
        answers_a = []
    try:
        answers_aaaa = [r.address for r in dns.resolver.resolve(domain, 'AAAA', lifetime=5)]
    except Exception:
        answers_aaaa = []
    try:
        answers_mx = [r.exchange.to_text() for r in dns.resolver.resolve(domain, 'MX', lifetime=5)]
    except Exception:
        answers_mx = []
    return {
        "A": answers_a,
        "AAAA": answers_aaaa,
        "MX": answers_mx
    }

def get_ssl_info(domain):
    # Checks for SSL, returns (supported, expired, days_left, not_after, subject)
    context = ssl.create_default_context()
    try:
        with socket.create_connection((domain, 443), timeout=5) as sock:
            with context.wrap_socket(sock, server_hostname=domain) as ssock:
                cert = ssock.getpeercert()
                not_after = cert['notAfter']
                subject = dict(x[0] for x in cert['subject'])
                expires = datetime.strptime(not_after, "%b %d %H:%M:%S %Y %Z")
                days_left = (expires - datetime.utcnow()).days
                expired = days_left < 0
                return {
                    "ssl_supported": True,
                    "ssl_expired": expired,
                    "ssl_days_left": days_left,
                    "ssl_not_after": not_after,
                    "ssl_subject": subject.get('commonName', '')
                }
    except Exception:
        return {
            "ssl_supported": False,
            "ssl_expired": None,
            "ssl_days_left": None,
            "ssl_not_after": None,
            "ssl_subject": None
        }

def get_cdn_info(domain):
    # Try homepage request, look for CDN headers
    try:
        resp = requests.get("https://" + domain, timeout=7)
    except Exception:
        try:
            resp = requests.get("http://" + domain, timeout=7)
        except Exception:
            return "Unknown"
    server = resp.headers.get('server', '').lower()
    headers = {k.lower():v.lower() for k,v in resp.headers.items()}
    # Known CDNs
    if 'cf-cache-status' in headers or 'cloudflare' in server:
        return "Cloudflare"
    if 'akamai' in server or 'akamai' in headers.get('x-cache', ''):
        return "Akamai"
    if 'fastly' in server or 'fastly' in headers.get('x-served-by', ''):
        return "Fastly"
    if 'incapsula' in server:
        return "Incapsula"
    if 'stackpath' in server or 'stackpath' in headers.get('x-cdn', ''):
        return "StackPath"
    return server.capitalize() if server else "Unknown"

def get_sitemap(domain):
    url = f"https://{domain}/sitemap.xml"
    try:
        resp = requests.get(url, timeout=7)
        if resp.status_code == 200 and resp.text.strip().startswith('<?xml'):
            return url
    except Exception:
        pass
    return None

def get_robots(domain):
    url = f"https://{domain}/robots.txt"
    try:
        resp = requests.get(url, timeout=5)
        if resp.status_code == 200:
            return resp.text
    except Exception:
        pass
    return None

def is_crawler_disallowed(robots_txt):
    # Very basic: look for Disallow for all or for our UA
    if robots_txt:
        lines = robots_txt.lower().splitlines()
        user_agent = None
        disallowed = False
        for l in lines:
            if l.startswith("user-agent:"):
                user_agent = l.split(":",1)[1].strip()
            elif l.startswith("disallow:"):
                path = l.split(":",1)[1].strip()
                if (user_agent in ('*', '', 'python', 'python-requests')) and (path == "/" or path.startswith("/")):
                    disallowed = True
        return disallowed
    return False

def crawl_site(start_url, domain, session_id):
    state = crawl_states[session_id]
    state["logs"].append(f"üåê Starting crawl: {start_url}")

    # --- Get domain-level info up front ---
    state["summary"] = {}
    # WHOIS/DNS/SSL/CDN
    domain_info = get_domain_info(domain)
    dns_info = get_dns_records(domain)
    ssl_info = get_ssl_info(domain)
    cdn = get_cdn_info(domain)
    sitemap = get_sitemap(domain)
    robots_txt = get_robots(domain)
    crawler_disallowed = is_crawler_disallowed(robots_txt)

    state["summary"].update({
        "domain_info": domain_info,
        "dns_info": dns_info,
        "ssl_info": ssl_info,
        "cdn": cdn,
        "sitemap": sitemap,
        "robots_txt": robots_txt,
        "crawler_disallowed": crawler_disallowed,
    })

    # --- Crawl site as before, now with images, title/meta, etc ---
    visited = set()
    to_visit = [start_url]
    outbound_links = {}    # url -> set(source_pages)
    broken_links = {}      # url -> set(source_pages)
    broken_images = []     # {"img": ..., "page": ...}
    pages_scanned = 0
    page_titles = {}       # url -> title
    meta_descs = {}        # url -> meta description

    state["max_progress_seen"] = 0

    should_cancel = lambda: crawl_states.get(session_id, {}).get("cancel", False)

    while to_visit and not should_cancel():
        url = to_visit.pop(0)
        if url not in visited:
            try:
                response = requests.get(url, timeout=10)
                if response.status_code == 404:
                    broken_links.setdefault(url, set()).add(url)
                    state["logs"].append(f"‚ùå 404 Not Found: {url} (found on {url})")
                    continue
                soup = BeautifulSoup(response.text, "html.parser")
                visited.add(url)
                state["logs"].append(f"‚úÖ Scanned: {url}")

                # Get page title & meta
                title = soup.title.string.strip() if soup.title and soup.title.string else ""
                page_titles[url] = title
                meta = soup.find('meta', attrs={'name': 'description'})
                meta_desc = meta['content'].strip() if meta and meta.get('content') else ""
                meta_descs[url] = meta_desc

                # Check images
                for img in soup.find_all("img", src=True):
                    img_href = urljoin(url, img['src'].strip())
                    try:
                        img_resp = requests.get(img_href, timeout=5)
                        if img_resp.status_code != 200:
                            broken_images.append({"img": img_href, "page": url})
                            state["logs"].append(f"üñºÔ∏è Broken image: {img_href} (found on {url})")
                    except Exception:
                        broken_images.append({"img": img_href, "page": url})
                        state["logs"].append(f"üñºÔ∏è Broken image: {img_href} (found on {url})")

                # Scan <a> tags
                for a in soup.find_all("a", href=True):
                    href = a['href'].strip()
                    if href == "#" or href.lower().startswith(("javascript:", "tel:")):
                        continue
                    if href.startswith("mailto:"):
                        try:
                            email = href.split(':',1)[1].split('?',1)[0]
                            email_domain = email.split('@')[1].lower()
                            if normalize_domain(email_domain) != domain:
                                already_seen = outbound_links.setdefault(href, set())
                                if url not in already_seen:
                                    already_seen.add(url)
                                    state["logs"].append(f"üìß External mailto: {href} (found on {url})")
                        except Exception:
                            already_seen = outbound_links.setdefault(href, set())
                            if url not in already_seen:
                                already_seen.add(url)
                                state["logs"].append(f"üìß Malformed mailto: {href} (found on {url})")
                        continue

                    raw_url = urldefrag(urljoin(url, href))[0]
                    parsed = urlparse(raw_url)
                    netloc = normalize_domain(parsed.netloc)
                    normalized_url = parsed._replace(netloc=netloc).geturl()

                    if netloc == "" or domain in netloc:
                        if normalized_url not in visited and normalized_url not in to_visit:
                            to_visit.append(normalized_url)
                    else:
                        if any(skip in netloc for skip in EXCLUDED_DOMAINS):
                            continue
                        outbound_links.setdefault(normalized_url, set()).add(url)
                        state["logs"].append(f"üîó Outbound: {normalized_url} (found on {url})")
                        try:
                            ext_resp = requests.get(normalized_url, timeout=5)
                            if ext_resp.status_code == 404:
                                broken_links.setdefault(normalized_url, set()).add(url)
                                state["logs"].append(f"‚ùå 404 External: {normalized_url} (found on {url})")
                        except Exception:
                            broken_links.setdefault(normalized_url, set()).add(url)
                            state["logs"].append(f"‚ùå Failed to load: {normalized_url} (found on {url})")

                pages_scanned += 1
                progress = int((pages_scanned / (pages_scanned + len(to_visit))) * 100) if (pages_scanned + len(to_visit)) else 100
                state["max_progress_seen"] = max(state.get("max_progress_seen", 0), progress)
                state["progress"] = state["max_progress_seen"]
                state["pages_scanned"] = pages_scanned

            except Exception:
                broken_links.setdefault(url, set()).add(url)
                state["logs"].append(f"‚ùå Failed to crawl: {url} (found on {url})")

        state["visited"] = list(visited)
        state["outbound_links"] = {k: list(v) for k, v in outbound_links.items()}
        state["broken_links"] = {k: list(v) for k, v in broken_links.items()}
        state["broken_images"] = broken_images
        state["page_titles"] = page_titles
        state["meta_descs"] = meta_descs

    # Duplicate Content Detection (simple: same title+meta)
    duplicate_content = []
    seen = {}
    for url, title in page_titles.items():
        meta = meta_descs.get(url, "")
        key = (title.strip().lower(), meta.strip().lower())
        if key in seen:
            duplicate_content.append((seen[key], url))
        else:
            seen[key] = url
    state["duplicate_content"] = duplicate_content

    # --- End summary ---
    state["logs"].append("‚úÖ Crawl complete.")
    num_outbound = len(outbound_links)
    num_broken = len(broken_links)
    num_broken_images = len(broken_images)
    num_dupes = len(duplicate_content)
    if num_outbound == 0 and num_broken == 0 and num_broken_images == 0 and num_dupes == 0:
        state["logs"].append("üéâ No incorrect outbound links, 404 errors, broken images, or duplicate content found.")
    else:
        state["logs"].append(f"Incorrect Outbound Links: {num_outbound}")
        state["logs"].append(f"404 Errors: {num_broken}")
        state["logs"].append(f"Broken Images: {num_broken_images}")
        state["logs"].append(f"Duplicate Content: {num_dupes}")

    # --- Summary Block ---
    state["logs"].append("\n=== Summary Report ===")
    state["logs"].append(f"Sitemap.xml: {'Found' if sitemap else 'Not found'}")
    if sitemap: state["logs"].append(f" - {sitemap}")
    state["logs"].append(f"Robots.txt: {'Found' if robots_txt else 'Not found'}")
    if robots_txt:
        state["logs"].append("---- robots.txt ----")
        state["logs"].append(robots_txt[:1000] + ("..." if len(robots_txt)>1000 else ""))
        state["logs"].append("--------------------")
        if crawler_disallowed:
            state["logs"].append("‚ö†Ô∏è Warning: robots.txt may disallow crawling of this site.")
    state["logs"].append(f"SSL Supported: {'Yes' if ssl_info['ssl_supported'] else 'No'}")
    if ssl_info['ssl_supported']:
        if ssl_info['ssl_expired']:
            state["logs"].append(f"‚ö†Ô∏è SSL Certificate Expired on {ssl_info['ssl_not_after']}")
        else:
            state["logs"].append(f"SSL Certificate Valid. Expires: {ssl_info['ssl_not_after']} ({ssl_info['ssl_days_left']} days left)")
    state["logs"].append(f"CDN: {cdn}")
    # DNS
    for typ, recs in dns_info.items():
        state["logs"].append(f"DNS {typ}: {', '.join(recs) if recs else 'None'}")
    # WHOIS
    di = domain_info
    state["logs"].append(f"Registrar: {di.get('registrar')}")
    state["logs"].append(f"Org: {di.get('org')}")
    state["logs"].append(f"Created: {di.get('created')}")
    state["logs"].append(f"Expires: {di.get('expires')}")
    state["logs"].append(f"Nameservers: {', '.join(di.get('nameservers', []))}")

    # Broken Images
    if broken_images:
        state["logs"].append("Broken images:")
        for img in broken_images:
            state["logs"].append(f" - {img['img']} (on {img['page']})")
    # Duplicates
    if duplicate_content:
        state["logs"].append("Duplicate Content Pages (title+meta):")
        for u1, u2 in duplicate_content:
            state["logs"].append(f" - {u1} and {u2}")

    state["progress"] = 100
    state["finished"] = True

@app.route("/", methods=["GET"])
def index():
    return render_template("index.html")

@app.route("/start_crawl", methods=["POST"])
def start_crawl():
    url = request.form.get("url", "").strip()
    if not url.startswith("http"):
        url = "https://" + url
    parsed = urlparse(url)
    domain = normalize_domain(parsed.netloc)
    session_id = session.get("sid") or str(time.time()) + str(os.getpid())
    session["sid"] = session_id

    # 1. DNS existence check
    try:
        socket.gethostbyname(domain)
    except socket.error:
        return jsonify({"status": "error", "msg": "‚ùå Domain does not exist or is unreachable!"}), 400

    # 2. WHOIS info
    domain_info = get_domain_info(domain)

    crawl_states[session_id] = {
        "logs": [],
        "progress": 0,
        "pages_scanned": 0,
        "visited": [],
        "outbound_links": {},
        "broken_links": {},
        "broken_images": [],
        "duplicate_content": [],
        "finished": False,
        "cancel": False,
        "start_url": url,
        "domain": domain,
        "domain_info": domain_info,
        "max_progress_seen": 0,
        "summary": {},
        "page_titles": {},
        "meta_descs": {}
    }

    threading.Thread(target=crawl_site, args=(url, domain, session_id), daemon=True).start()
    return jsonify({"status": "started", "domain_info": domain_info})

@app.route("/progress", methods=["GET"])
def progress():
    session_id = session.get("sid")
    state = crawl_states.get(session_id)
    if not state:
        return jsonify({"logs": [], "progress": 0, "pages_scanned": 0, "finished": True, "domain_info": {}})
    # Add domain_info, summary
    resp = {
        "logs": state["logs"],
        "progress": state["progress"],
        "pages_scanned": state["pages_scanned"],
        "finished": state["finished"],
        "domain_info": state.get("domain_info", {}),
        "summary": state.get("summary", {})
    }
    return jsonify(resp)

@app.route("/cancel", methods=["POST"])
def cancel():
    session_id = session.get("sid")
    state = crawl_states.get(session_id)
    if state:
        state["cancel"] = True
        state["logs"].append("‚èπÔ∏è Crawl cancelled by user.")
    return jsonify({"status": "cancelled"})

@app.route("/export", methods=["GET"])
def export():
    session_id = session.get("sid")
    state = crawl_states.get(session_id)
    if not state:
        return "No data", 404
    domain_info = state.get("domain_info", {})
    summary = state.get("summary", {})
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    domain_clean = state["domain"].replace('.', '_')
    filename = f"crawl_results_{domain_clean}_{timestamp}.txt"
    with open(filename, "w", encoding="utf-8") as f:
        f.write("Terry Ecom Link Checker Results\n")
        f.write(f"Checked Domain: {domain_info.get('domain', state['domain'])}\n")
        f.write(f"Created: {domain_info.get('created')}\n")
        f.write(f"Expires: {domain_info.get('expires')}\n")
        f.write(f"Org: {domain_info.get('org')}\n")
        f.write(f"Registrar: {domain_info.get('registrar')}\n")
        f.write(f"Nameservers: {', '.join(domain_info.get('nameservers', []))}\n")
        # DNS/SSL
        dns_info = summary.get('dns_info', {})
        for typ, recs in dns_info.items():
            f.write(f"DNS {typ}: {', '.join(recs) if recs else 'None'}\n")
        ssl_info = summary.get('ssl_info', {})
        f.write(f"SSL Supported: {ssl_info.get('ssl_supported')}\n")
        if ssl_info.get('ssl_supported'):
            f.write(f"SSL Certificate Expires: {ssl_info.get('ssl_not_after')}\n")
        f.write(f"CDN: {summary.get('cdn')}\n")
        f.write(f"Sitemap.xml: {summary.get('sitemap')}\n")
        f.write(f"Robots.txt: {('Disallowed' if summary.get('crawler_disallowed') else 'Allowed')}\n")
        f.write(f"Timestamp: {timestamp}\n\n")
        f.write("External/Malformed Mailto Links (and where found):\n")
        for link, sources in sorted(state["outbound_links"].items()):
            for src in sources:
                f.write(f"{link} (found on {src})\n")
        f.write("\nBroken Links (404s) and where found:\n")
        for link, sources in sorted(state["broken_links"].items()):
            for src in sources:
                f.write(f"{link} (found on {src})\n")
        f.write("\nBroken Images (and where found):\n")
        for img in state.get("broken_images", []):
            f.write(f"{img['img']} (on {img['page']})\n")
        f.write("\nDuplicate Content Pages:\n")
        for u1, u2 in state.get("duplicate_content", []):
            f.write(f"{u1} and {u2}\n")
        f.write("\nFull Crawl Log:\n")
        for line in state["logs"]:
            f.write(line + "\n")
    return send_file(filename, as_attachment=True)

if __name__ == "__main__":
    app.run(debug=True)
